{
  "days": [
    {
      "date": "May 6",
      "title": "Tuesday May 6",
      "url": "/schedule/"
    },
    {
      "date": "May 7",
      "title": "Wednesday May 7",
      "url": "/schedule-day-2/"
    }
  ],
  "categories": [
    {
      "name": "AI Model",
      "room": "Master Stage",
      "id": "ai-model"
    },
    {
      "name": "AI Infra",
      "room": "Central Room",
      "id": "ai-infra"
    },
    {
      "name": "AI Apps",
      "room": "Open Stage",
      "id": "ai-apps"
    },
    {
      "name": "Embodied AI",
      "room": "Junior Stage",
      "id": "embodied-ai"
    },
    {
      "name": "PyTorch Day France",
      "room": "Founders Café",
      "id": "pytorch"
    },
    {
      "name": "GOSIM AI Spotlight",
      "room": "Open Platform",
      "id": "spotlight-ai"
    }
  ],
  "sessions": {
    "AI Model": [
      {
        "date": "May 6",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "10:30 - 11:10",
        "title": "Open-R1: A Fully Open Reproduction of DeepSeek-R1",
        "content": "The recipe behind OpenAI’s reasoning models has been a well kept secret. That is, until earlier this year, when DeepSeek released their DeepSeek-R1 model and promptly broke the internet. While a detailed technical report was published, many open questions remain, chief among them the training data, which was not released. Open-R1 is Hugging Face's fully open effort to replicate DeepSeek-R1, with a strong focus on reasoning data curation.",
        "speakers": [
          {
            "id": "guilherme-penedo",
            "name": "Guilherme Penedo",
            "tags": ["ai-model"],
            "roleOrg": "ML Research Engineer at Hugging Face",
            "image": "guilherme-penedo.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "11:10 - 11:50",
        "title": "OpenSeek: Collaborative Innovation for Next-Gen Models",
        "content": "OpenSeek aims to unite the global open-source community to drive collaborative innovation in algorithms, data, and systems to develop next-generation models that surpass DeepSeek.",
        "speakers": [
          {
            "id": "guang-liu",
            "name": "Guang Liu",
            "tags": ["ai-model"],
            "roleOrg": "Technical Lead of Data Research Team at BAAI",
            "image": "guang-liu.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "12:30 - 13:50",
        "title": "Spotlight Talks",
        "content": "GOSIM AI Spotlight Finalists will present their projects.",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "14:00 - 14:40",
        "title": "Decode DeepSeek: the Technological Innovation and Its Influence on AI Ecosystem",
        "content": "Recently, DeepSeek has attracted a great deal of attention with its outstanding technological innovations and is set to have a profound and extensive impact on the AI industry. This speech is divided into two parts. In the first part, we will  top-down walkthrough of DeepSeek’s technological innovations, including the paradigm shift in inference computing led by its open-source reinforcement learning solution, innovations in model architecture such as MLA and MOE, and performance optimizations in system engineering. In the second part, we will explore the transformations and impacts on global AI ecosystem triggered by DeepSeek, including its key influences on aspects such as AI applications、AI Agents, the computing power landscape, and AI open-source initiatives.",
        "speakers": [
          {
            "id": "jason-li",
            "name": "Jason Li",
            "tags": ["ai-model"],
            "roleOrg": "Senior VP at CSDN",
            "image": "jason-li.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "14:40 - 15:20",
        "title": "Linear Next: The Evolution of LLM Architecture",
        "content": "The Transformer architecture, despite its popularity, suffers from quadratic computational complexity. Recent advances in computing hardware, such as the V100 to H200 series, have temporarily alleviated this issue, reducing the immediate need for alternatives in the industry. Linear-complexity solutions for large models are still in the research phase, lacking widespread validation in practical applications. Consequently, Transformer remains the preferred choice.\r\n\r\nHowever, as improvements in computing power slow down, the demand for architectures that surpass Transformer in efficiency will grow. Our team has developed Lightning Attention, a novel mechanism based on linear attention. By rearranging the QKV multiplication order (Q(KV)), Lightning Attention achieves linear computational complexity relative to sequence length. Experiments show it significantly outperforms the latest Transformers in both efficiency and performance, validated on a 456B MoE model (MiniMax 01). This innovation paves the way for more efficient large language models, offering new possibilities for future development.",
        "speakers": [
          {
            "id": "yiran-zhong",
            "name": "Yiran Zhong",
            "tags": ["ai-model"],
            "roleOrg": "Senior Research Director at MiniMax",
            "image": "yiran-zhong.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "15:40 - 16:20",
        "title": "The Curse of Depth in Large Language Models",
        "content": "Large Language Models (LLMs) have demonstrated impressive achievements. However, recent research has shown that their deeper layers often contribute minimally, with effectiveness diminishing as layer depth increases. This pattern presents significant opportunities for model compression. In the first part of this seminar, we will explore how this phenomenon can be harnessed to improve the efficiency of LLM compression. Despite these opportunities, the underutilization of deeper layers leads to inefficiencies, wasting resources that could be better used to enhance model performance. The second part of the talk will address the root cause of this ineffectiveness in deeper layers and propose a solution. We identify the issue as stemming from the prevalent use of Pre-Layer Normalization (Pre-LN) and introduce LayerNorm Scaling to solve this issue. ",
        "speakers": [
          {
            "id": "shiwei-liu",
            "name": "Shiwei Liu",
            "tags": ["ai-model"],
            "roleOrg": "Royal Society Newton International Fellow at University of Oxford",
            "image": "shiwei-liu.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "16:20 - 17:00",
        "title": "Going Beyond Tokens for Code Large Language Models",
        "content": "Tokenization in LLMs is the last bit of clunkiness in an otherwise elegant, highly-optimized architecture. This talk presents interesting avenues in tokenizer-free architecture to go \"beyond tokens\" in order to reduce latency and improve performance.",
        "speakers": [
          {
            "id": "diego-rojas",
            "name": "Diego Rojas",
            "tags": ["ai-model"],
            "roleOrg": "Research Engineer at Zhipu.AI",
            "image": "diego-rojas.jpeg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "17:00 - 17:40",
        "title": "How to Build Competitive Large Language Models \"Made in Europe\"?",
        "content": "How to Build Competitive Large Language Models “Made in Europe”\r\n\r\nAs Europe accelerates its efforts to develop sovereign AI capabilities, initiatives like OpenEuroLLM, OpenGPT-X, TrustLLM, and EuroLingua-GPT are leading the charge in creating multilingual, open-source large language models (LLMs) that reflect European values. This talk will delve into the insights gained from these projects, highlighting the challenges and breakthroughs in training competitive LLMs within Europe's unique regulatory and linguistic landscape.​\r\nWe'll explore how research initiatives are addressing key issues such as data diversity, computational infrastructure, and model transparency with the goal of gaining an understanding of the lay of the land of Europe's AI advancements.​\r\n\r\nJoin this talk to get a glimpse of how Europe's commitment to openness, trustworthiness, and cultural inclusivity is shaping the next generation of AI, and learn how you can contribute to this transformative journey.",
        "speakers": [
          {
            "id": "nicolas-flores-herr",
            "name": "Nicolas Flores-Herr",
            "tags": ["ai-model"],
            "roleOrg": "Team Lead Foundation Models at Fraunhofer IAIS",
            "image": "nicolas-flores-herr.jpeg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "17:00 - 18:00",
        "title": "Spotlight Demos",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:30 - 11:10",
        "title": "Multilingualism of Qwen: From Foundation Model to Applications",
        "content": "Multilingual and cross-lingual capabilities significantly boost the flexibility and usefulness of large language models (LLMs). Using Qwen as an example, we'll explore methods to enhance multilingual performance in LLMs, including pre-training, post-training, and evaluation strategies. Additionally, we'll examine the real-world applications of these advancements, demonstrating how multilingual capabilities can create practical solutions that overcome language barriers and promote smooth communication.",
        "speakers": [
          {
            "id": "baosong-yang",
            "name": "Baosong Yang",
            "tags": ["ai-model"],
            "roleOrg": "Scientist at Alibaba's Tongyi Lab",
            "image": "baosong-yang.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "11:10 - 11:50",
        "title": "Open Foundation Models: Scaling Laws and Generalization",
        "content": "To study transferable learning and generalization, derivation of reproducible scaling laws is crucial. We highlight why open foundation models and datasets are essential for this research and highlight challenges in properly measuring generalization.",
        "speakers": [
          {
            "id": "jenia-jitsev",
            "name": "Jenia Jitsev",
            "tags": ["ai-model"],
            "roleOrg": "Scientific lead at LAION; Juelich Supercomputing Center, Research Center Juelich",
            "image": "jenia-jitsev.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "12:30 - 13:50",
        "title": "Spotlight Talks",
        "content": "GOSIM AI Spotlight Finalists will present their projects.",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "14:00 - 14:40",
        "title": "LUCIE : From Scratch with Love !",
        "content": "LUCIE-7B is one of the first large language models that’s truly open, from training code to dataset. This talk shares how we built it using 512 GPUs, a multilingual tokenizer, and public data. We explain the challenges, the lessons, and the reasons behind our open approach. OpenLLM France is building a European AI ecosystem based on transparency and trust. In 2025, we’ll push further: larger models, multimodality, and on-device AI for all.",
        "speakers": [
          {
            "id": "michel-marie-maudet",
            "name": "Michel-Marie Maudet",
            "tags": ["ai-model"],
            "roleOrg": "General Manager at LINAGORA - OpenLLM France",
            "image": "michel-marie-maudet.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "14:40 - 15:20",
        "title": "Demysifying LLM Training --- Towards Fully Open-source LLM from Pre-training to Reinforcement Learning",
        "content": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community for their power and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across various applications. Although LLMs offer unprecedented opportunities for research and innovation, its commercialization has raised concerns about transparency, reproducibility, and safety. Many open LLM models lack the necessary components (such as training code and data) for full understanding and reproducibility, and some use restrictive licenses whilst claiming to be “open-source”, which may hinder further innovations on LLMs. To mitigate this issue, we follow the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. We present a truly open source LLM Moxin 7B and release pre-training code and configurations, training and fine-tuning data, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. We also finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization, an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model, leading to the Moxin Reasoning model.",
        "speakers": [
          {
            "id": "yanzhi-wang",
            "name": "Yanzhi Wang",
            "tags": ["ai-model"],
            "roleOrg": "Professor, Electrical and Computer Engineering at Northwestern University",
            "image": "yanzhi-wang.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "15:40 - 16:20",
        "title": "Small but Mighty: How MiniCPM Made Breakthroughs in the Global Open-Source AI Landscape",
        "content": "MiniCPM, nicknamed 'ModelBest's Little Steel Cannon'—which includes the large language model MiniCPM and the multimodal large model MiniCPM-V—has gained widespread recognition in the global AI community due to its highly efficient and cost-effective nature, embodying the principle of 'punching above its weight' These projects have cumulatively received over 26,000 stars on GitHub, with total downloads exceeding 7 million across the web, becoming benchmark works in the field of on-device AI.",
        "speakers": [
          {
            "id": "chao-jia",
            "name": "Chao Jia",
            "tags": ["ai-model"],
            "roleOrg": "Founding Partner at OpenBMB & Modelbest",
            "image": "chao-jia.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "16:20 - 17:00",
        "title": "Pre-training of Smol and Large LLM",
        "content": "Explaining what's new in pre-training: optimization tricks, MoE, stability hacks, and handling long contexts—everything you need to build better LLMs.",
        "speakers": [
          {
            "id": "elie-bakouch",
            "name": "Elie Bakouch",
            "tags": ["ai-model"],
            "roleOrg": "Research Engineer at Hugging Face",
            "image": "elie-bakouch.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "17:00 - 18:00",
        "title": "Spotlight Demos",
        "content": "",
        "speakers": []
      }
    ],
    "AI Infra": [
      {
        "date": "May 6",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "10:30 - 11:10",
        "title": "AI Open Source for Good: Inclusive Access, Equitable Data, and Accessible Compute",
        "content": "This talk unveils how open source technologies act as catalysts for equitable AI across three pillars. First, inclusive access: We open-source voice datasets tailored for underrepresented groups—such as children and the elderly—to ensure multimodal AI systems understand diverse linguistic patterns and bridge generational divides. Second, equitable data: we have released nearly 100 globally accessible datasets, amassing over 680,000 downloads, empowering developers from any countries to innovate freely. Third, accessible compute: We present FlagOS, an open-source system software that facilitates AI development and deployment across diverse hardware ecosystems—including legacy GPUs and emerging accelerators—while significantly lowering the cost barrier to AI innovation. Collectively, these open-source efforts transform 'AI for Good' into a shared mission—breaking barriers of age, location, and resources to empower anyone to create and benefit from AI.",
        "speakers": [
          {
            "id": "yonghua-lin",
            "name": "Yonghua Lin",
            "tags": ["ai-infra"],
            "roleOrg": "VP of BAAI",
            "image": "yonghua-lin.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "11:10 - 11:50",
        "title": "The Best Practice of Training and Inferencing on Ascend CANN",
        "content": "The AI-oriented, heterogeneous Compute Architecture for Neural Networks (CANN) is a key platform for improving the computing efficiency of Ascend AI processors. It serves as a bridge between upper-layer AI frameworks and lower-layer AI processors and programming. This topic will focus on OpenSource ecosystem about CANN, shows how CANN helps AI sofeware, such as pytorch, vllm and so on, efficiently running on Ascend.",
        "speakers": [
          {
            "id": "xiyuan-wang",
            "name": "Xiyuan Wang",
            "tags": ["ai-infra"],
            "roleOrg": "Senior Software Engineer at Huawei",
            "image": "xiyuan-wang.jpeg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "12:30 - 13:50",
        "title": "Spotlight Talks",
        "content": "GOSIM AI Spotlight Finalists will present their projects.",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "14:00 - 14:40",
        "title": "Make Your LLMs Serverless",
        "content": "LLMs require GPUs, causing scarcity. Overprovisioning them is expensive and a waste. Google Cloud Run now offers serverless GPU support, enabling cost-effective LLM deployment. A live demo will compare Gemma model performance with and without GPUs.",
        "speakers": [
          {
            "id": "guillaume-blaquiere",
            "name": "Guillaume Blaquiere",
            "tags": ["ai-infra"],
            "roleOrg": "Group Data Architect at Carrefour",
            "image": "guillaume-blaquiere.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "14:40 - 15:20",
        "title": "Open-source Intelligent Computing Integrated Management and Utilization Foundational Software - SCOW and CraneSched",
        "content": "The Peking University Computing Center is dedicated to developing general foundational software for both supercomputing (HPC) and intelligent computing (AI computing). In the field of HPC and AI computing, it has developed several flagship foundational software systems, including SCOW and CraneSched.\r\nOpenSCOW (https://github.com/PKUHPC/OpenSCOW) provides a graphical user interface (GUI) that allows developers to flexibly manage supercomputing and AI computing resources for AI model training and inference. It has already been deployed across 56 computing centers, including 34 universities and 12 enterprises in China.\r\nCraneSched ( https://github.com/PKUHPC/CraneSched) is a high-performance scheduling and orchestration system for HPC and AI computing tasks. It supports large-scale model training with exceptional performance and has been adopted by 8 universities and 1 enterprise in China.",
        "speakers": [
          {
            "id": "yinping-ma",
            "name": "Yinping Ma",
            "tags": ["ai-infra"],
            "roleOrg": "Engineer at Peking University",
            "image": "yinping-ma.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "15:40 - 16:20",
        "title": "verl: Hybrid Controller-based RLHF System",
        "content": "verl is a flexible, efficient and production-ready RL training library for LLMs. This talk will share the ideas in designing a hybrid-controller system and the benefits of this system in efficient large-scale RL training.",
        "speakers": [
          {
            "id": "yaowei-zheng",
            "name": "Yaowei Zheng",
            "tags": ["ai-infra"],
            "roleOrg": "Ph.D. Student at Beihang University",
            "image": "yaowei-zheng.jpeg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "16:20 - 17:00",
        "title": "Datasets and Infrastructure for DeepSeek-R1 Style Reinforcement Learning (GRPO)",
        "content": "We will walk through everything you need to know about the latest in reinforcement learning for LLMs, datasets and infrastructure, down to training your own small reasoning LLM that can write code locally. ",
        "speakers": [
          {
            "id": "greg-schoeninger",
            "name": "Greg Schoeninger",
            "tags": ["ai-infra"],
            "roleOrg": "CEO at Oxen.ai",
            "image": "greg-schoeninger.jpeg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "17:00 - 18:00",
        "title": "Spotlight Demos",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:30 - 11:10",
        "title": "Streamlining AI App Development with Docker: Models and AI Tools That Just Work",
        "content": "Discover how Docker’s Model Runner enables fast, local AI inference with GPU support, and how the Docker makes it easy to integrate LLMs and agents using MCP —no complex setup required.",
        "speakers": [
          {
            "id": "jeanlaurent-de-morlhon",
            "name": "Jean-Laurent de Morlhon",
            "tags": ["ai-infra"],
            "roleOrg": "Sr Vice President, GenAI Acceleration at Docker ",
            "image": "jean-laurent-de-morlhon.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "11:10 - 11:50",
        "title": "Data Spaces as the Backbone for an AI Dataverse",
        "content": "A sound ecosystem of AI, LLMs, and Agentic AI is essential for the AI and Data economy. This ecosystem requires an additional layer of trust to connect data and infrastructures through marketplaces and guarantee data governance and usage control, among other things. This is necessary to meet the diverse data requirements in the EU, including the GDPR, the Data Act, and the Data Governance Act, among others. Also, the interconnection via self-sovereign identity may allow different partners to add extra trust interactions.\r\nThis is provided by the data spaces and the trusted data matrix initiates all over the world.\r\nIn the EUNOMIA project, we are integrating the MCP protocol ( Model Context Protocol ) as a way to feed data from the flourishing European data spaces infrastructure to the LLMs. This could be interconnected, in a trusted way, with different partners via the Gaia-X trust framework, the EIDAS 2 initiative and the self-sovereign identity infrastructure.\r\nThis could be generalized into  the new A2A protocol, presented by Google. In fact, this protocol could be merged with our Advance data space protocol in order to add extra trust capabilities. So this could be a perfect match since the Authentication and authorization for actuation over real systems.\r\nThis could lead to an architecture of a fully distributed trusted architecture between different actors all over the world that could lead to a Dataverse upon which different applications and services may be automatically deployed that will be introduced.",
        "speakers": [
          {
            "id": "joaquin-salvachua",
            "name": "Joaquin Salvachua",
            "tags": ["ai-infra"],
            "roleOrg": "Profesor Titular at Universidad Politecnica de Madrid",
            "image": "joaquin-salvachua.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "12:30 - 13:50",
        "title": "Spotlight Talks",
        "content": "GOSIM AI Spotlight Finalists will present their projects.",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "14:00 - 14:40",
        "title": "Kubetorch: A Modern Kubernetes-Native Approach to ML Execution and Production",
        "content": "Mature organizations run ML workloads on Kubernetes, but implementations vary widely, and ML engineers rarely enjoy the streamlined development and deployment experiences that platform engineering teams provide for software engineers. Making small changes takes an hour to test and moving from research to production frequently takes multiple weeks – these unergonomic and inefficient processes are unthinkable for software, but standard in ML. Kubetorch is an introduction of a novel compute platform that is Kubernetes-native that offers a great, iterable, and debuggable interface into powerful compute for developers, without introducing new pitfalls of brittle infrastructure or long deployment times. ",
        "speakers": [
          {
            "id": "paul-yang",
            "name": "Paul Yang",
            "tags": ["ai-infra"],
            "roleOrg": "ML Engineering at Runhouse",
            "image": "paul-yang.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "14:40 - 15:20",
        "title": "RAGFlow: Leading the Open-Source Revolution in Enterprise-Grade Retrieval-Augmented Generation",
        "content": "RAGFlow tackles core RAG challenges—data quality, semantic gaps, low hit rates—with open-source solutions. This talk demonstrates enhanced retrieval, reasoning, and multimodal capabilities for robust enterprise AI applications.\r\n",
        "speakers": [
          {
            "id": "yingfeng-zhang",
            "name": "Yingfeng Zhang",
            "tags": ["ai-infra"],
            "roleOrg": "CEO of InfiniFlow",
            "image": "yingfeng-zhang.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "15:40 - 16:20",
        "title": "Khronos in the World of Open Source and Machine Learning",
        "content": "Discover the Khronos Group's vital role at the intersection of machine learning, open source, and open standards. Learn how Khronos, an open, non-profit, member-driven consortium, creates royalty-free standards for compute, graphics, ML, and more, fosters innovation and interoperability.\r\n\r\nThis talk explores Khronos's use of open source principles and highlights key Khronos APIs accelerating ML, including OpenCL, SYCL, OpenVX with NNEF, and the increasingly important Vulkan API for high-performance inference and cross-platform deployment.\r\n\r\nJoin this session for insights into how Khronos standards and open source collaboration are shaping the future of hardware-accelerated machine learning.",
        "speakers": [
          {
            "id": "markus-tavenrath",
            "name": "Markus Tavenrath",
            "tags": ["ai-infra"],
            "roleOrg": "Principal Engineer Developer Technology at NVIDIA",
            "image": "markus-tavenrath.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "16:20 - 17:00",
        "title": "WGML: the Story of Building a New High-performance, Cross-platform, On-device Inference Framework.",
        "content": "Ever dreamed of writing a new low-level LLM inference library? Check out the making of WGML, an open-source high-performance cross-platform GPU inference framework using Rust and WebGPU. We will cover tips for discovering and implementing LLM inference.",
        "speakers": [
          {
            "id": "sebastien-crozet",
            "name": "Sebastien Crozet",
            "tags": ["ai-infra"],
            "roleOrg": "CEO at Dimforge",
            "image": "sebastien-crozet.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "17:00 - 18:00",
        "title": "Spotlight Demos",
        "content": "",
        "speakers": []
      }
    ],
    "AI Apps": [
      {
        "date": "May 6",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "10:30 - 11:10",
        "title": "TONGYI Lingma: from Coding Copilot to Coding Agent",
        "content": "This presentation will take the perspective of intelligent development in the software engineering to outline and introduce the latest technological advancements and product applications of Code LLMs, Coding Copilot, and Coding Agents, as well as analyze and forecast future development trends.",
        "speakers": [
          {
            "id": "yongbin-li",
            "name": "Yongbin Li",
            "tags": ["ai-apps"],
            "roleOrg": "Principal Research Scientist at Alibaba Tongyi Lab",
            "image": "yongbin-li.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "11:10 - 11:50",
        "title": "CangjieMagic : New Choices for Developers in the Age of Large Models",
        "content": "With the surge in popularity of various AI large models, the trend of agent-oriented development in large model applications has become increasingly evident. Agents are gradually becoming a core element in the development of large model applications. This topic shares an AI large model Agent development framework based on the Cangjie programming language. This framework supports Agent-oriented programming, providing developers with an efficient Agent DSL for Agent programming. Its main features include: support for the MCP protocol to facilitate mutual invocation between Agents and tools, support for modular invocation, and support for intelligent task planning. It enhances the efficiency of developers in creating smart HarmonyOS applications, delivers an exceptional development experience, and explores new paradigms for future large model application development",
        "speakers": [
          {
            "id": "dongjie-chen",
            "name": "Dongjie Chen",
            "tags": ["ai-apps"],
            "roleOrg": "Software Engineer at Huawei",
            "image": "dongjie-chen.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "12:30 - 13:50",
        "title": "Spotlight Talks",
        "content": "GOSIM AI Spotlight Finalists will present their projects.",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "14:00 - 14:40",
        "title": "Tech Together, Powered by Dify",
        "content": "Dify is a next-generation AI-native application development platform that bridges cutting-edge technology with real-world business value. Built on a robust open-source foundation, Dify integrates modern tech stacks—including LLM orchestration, RAG (Retrieval-Augmented Generation), fine-tuning tools, and multi-agent workflows—to simplify the creation, deployment, and scaling of AI applications.\r\n\r\nOur global developer community has become a hub for innovation, with thousands of contributors and enterprise adopters leveraging Dify to build everything from intelligent chatbots to enterprise-grade automation systems. This talk will highlight:\r\n\r\nDify’s open-source ecosystem and its role in accelerating AI adoption;\r\nKey technologies powering the platform and how they solve real-world challenges;\r\nSuccess stories from developers and enterprises.",
        "speakers": [
          {
            "id": "xinrui-liu",
            "name": "Xinrui Liu",
            "tags": ["ai-apps"],
            "roleOrg": "Developer Ecosystem Director at LangGenius",
            "image": "xinrui-liu.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "14:40 - 15:20",
        "title": "Using AI to Vibe Code Rust UI's for Mobile, Web and Mixed Reality",
        "content": "In this talk i will show vibecoding makepad UIs and UI shaders with Makepad Studio and an LLM. Makepad Studio is our visual design / code environment and the vision is to bring back Visual Basic, but now for a modern language: Rust. ",
        "speakers": [
          {
            "id": "rik-arends",
            "name": "Rik Arends",
            "tags": ["ai-apps"],
            "roleOrg": "Co-Founder of Makepad",
            "image": "rik-arends.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "15:40 - 16:20",
        "title": "Unifying AI Integration with Model Context Protocol",
        "content": "The Model Context Protocol (MCP) standardizes how AI models interact with external tools and resources through a structured client-server architecture, facilitating robust agent development. The engineering community worldwide keeps sharing MCP servers that enable client interactions that open truly remarkable and innovative applications.\r\n\r\nThis talk delves into MCP's core capabilities and how the MCP Java SDK combined with Spring AI MCP can integrate AI with your existing resources and applications.\r\n\r\nToday's intelligent agents can understand context, guide decisions, and integrate seamlessly with external services. Through live coding and practical examples, we will illustrate how to implement both client and server components. By attending this session, you will gain a practical understanding of MCP's standardized interfaces and architectural best practices, empowering you to build and extend AI-powered applications with agent-like capabilities.\r\n\r\nWhether you're developing new AI-driven solutions or enhancing existing systems, this talk will equip you with the tools and strategies needed to leverage MCP effectively.",
        "speakers": [
          {
            "id": "christian-tzolov",
            "name": "Christian Tzolov",
            "tags": ["ai-apps"],
            "roleOrg": "R&D Software Engineer at Spring team at Broadcom",
            "image": "christian-tzolov.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "16:20 - 17:00",
        "title": "The \"T\" in MCP and A2A Stands for Trust",
        "content": "This talk follows the spirit of Elena Cross's blog titled \"The \"S\" in MCP Stands for Security\". The challenges we face in AI agent deployment in the real world exceed what current security solutions can address. That is true even if you incorporate \"best practice\" security solutions by design and implement them competently. It's time that we move beyond security to think about trust instead. In this talk, we will examine what are still missing even if we implement common security in MCP and A2A, discuss what these new trust primitives are, and show how developers can adopt strong trust primitives integrated with MCP and A2A in agent based applications.",
        "speakers": [
          {
            "id": "wenjing-chu",
            "name": "Wenjing Chu",
            "tags": ["ai-apps"],
            "roleOrg": "Senior Director of Technology Strategy at Futurewei Technologies, Inc.",
            "image": "wenjing-chu.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "17:00 - 17:40",
        "title": "Cegid Pulse, Multi Agents Business Management Platform",
        "content": "Cegid is a global leader in cloud-based solutions for finance, HR, and retail, serving businesses of all sizes with tools to enhance efficiency and compliance. This talk introduces Cegid Pulse OS, a next-generation business management platform that enables seamless collaboration between human users and AI-powered agents. Leveraging large language models, Pulse OS dynamically generates and executes tasks through multi-agent conversations, where each agent—built with code or natural language—can interpret, act on, and respond to user intent. Designed to streamline operations, Pulse OS represents a shift toward intelligent, conversation-driven business workflows.",
        "speakers": [
          {
            "id": "hong-thai-nguyen",
            "name": "Hong-Thai Nguyen",
            "tags": ["ai-apps"],
            "roleOrg": "Software Engineering Manager at Cegid",
            "image": "hong-thai-nguyen.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "17:00 - 18:00",
        "title": "Spotlight Demos",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:30 - 11:10",
        "title": "OAKS: The Open Agentic AI Knowledge Stack",
        "content": "In this talk, we present an OSS AI architecture for Agentic AI+Knowledge.  Encapsulating business knowledge is key for agents, and focusing on AI memory and scalable frameworks around Knowledge Graphs is a good foundation to build an OSS AI ecosystem for agents.",
        "speakers": [
          {
            "id": "alexy-khrabrov",
            "name": "Alexy Khrabrov",
            "tags": ["ai-apps"],
            "roleOrg": "AI Community Architect at Neo4j",
            "image": "alexy-khrabrov.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "11:10 - 11:50",
        "title": "OpenManus: Empowering LLM-based Agent Applications Via Framework and Capability Evolution",
        "content": "We introduce OpenManus, a lightweight and versatile LLM-based multi-agent framework evolved from MetaGPT, designed to enhance adaptability, autonomy, and scalability through advanced reasoning, planning, and effective cross-environment operation.",
        "speakers": [
          {
            "id": "sirui-hong",
            "name": "Sirui Hong",
            "tags": ["ai-apps"],
            "roleOrg": "Co-Founder of DeepWisdom, MetaGPT/OpenManus Team Leader",
            "image": "sirui-hong.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "12:30 - 13:50",
        "title": "Spotlight Talks",
        "content": "GOSIM AI Spotlight Finalists will present their projects.",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "14:00 - 14:40",
        "title": "Rethinking the Open Source Agent Stack with Kagent",
        "content": "AI agents offer incredible promise, but are difficult to operationalize. Kagent delivers the full Kubernetes-native stack needed—beyond just a framework—to build, deploy, manage robust agents declaratively & reliably at scale.",
        "speakers": [
          {
            "id": "eitan-yarmush",
            "name": "Eitan Yarmush",
            "tags": ["ai-apps"],
            "roleOrg": "Senior Architect at solo.io",
            "image": "eitan-yarmush.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "14:40 - 15:20",
        "title": "Finding the Scaling Law of Agents",
        "content": "This talk explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named CAMEL. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents. In particular, we conduct comprehensive studies on cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https://github.com/camel-ai/camel.",
        "speakers": [
          {
            "id": "guohao-li",
            "name": "Guohao Li",
            "tags": ["ai-apps"],
            "roleOrg": "Founder and CEO of Eigent.AI / CAMEL-AI.org",
            "image": "guohao-li.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "15:40 - 16:20",
        "title": "Database for Agents Memory, The Right Way",
        "content": "In this session, we will explore best practices for leveraging serverless SQL databases to support the sophisticated memory requirements of AI agents. We will delve into essential technical requirements, including schema design considerations, efficient indexing strategies, consistency vs. availability trade-offs, handling real-time updates, and seamless integration with AI workflows. Additionally, we'll discuss common pitfalls, performance optimization techniques, and how to achieve cost-efficiency without sacrificing responsiveness or data integrity.\r\n\r\nAttendees will gain actionable insights into architecting robust, scalable memory storage solutions that enhance the capability, adaptability, and overall effectiveness of AI agents in production environments.",
        "speakers": [
          {
            "id": "dongxu-huang",
            "name": "Dongxu Huang",
            "tags": ["ai-apps"],
            "roleOrg": "CTO at PingCAP",
            "image": "dongxu-huang.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "16:20 - 17:00",
        "title": "Agentic Search",
        "content": "The talk covers basic concepts and use-cases of agentic search.",
        "speakers": [
          {
            "id": "florian-hnicke",
            "name": "Florian Hönicke",
            "tags": ["ai-apps"],
            "roleOrg": "Principal AI Engineer at Jina AI",
            "image": "florian-hnicke.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "17:00 - 18:00",
        "title": "Spotlight Demos",
        "content": "",
        "speakers": []
      }
    ],
    "Embodied AI": [
      {
        "date": "May 6",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "10:30 - 11:10",
        "title": "Mind, Body and Zenoh",
        "content": "As Robotics and Artificial Intelligence continue to evolve at an unprecedented pace, a critical gap has emerged in their ability to scale and operate seamlessly across diverse environments: the absence of an efficient, intelligent \"nervous system.\" Much like biological organisms rely on their nervous system to connect the body and the brain, autonomous systems require a foundational layer that enables real-time communication, adaptability, and distributed cognition. This talk introduces Zenoh, a cutting-edge Open Source protocol that is rapidly gaining traction in the robotics community and beyond. Zenoh bridges the traditional divide between data in motion, data at rest, and computation, enabling seamless data exchange from the edge to the cloud. Zenoh it's the missing link to unify sensing, actuation, and cognition. It is, in essence, the nervous system for the intelligent robots age.",
        "speakers": [
          {
            "id": "angelo-corsaro",
            "name": "Angelo Corsaro",
            "tags": ["embodied-ai"],
            "roleOrg": "CEO/CTO of ZettaScale",
            "image": "angelo-corsaro.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "11:10 - 11:50",
        "title": "Distributed Dataflows in Dora Using Zenoh",
        "content": "The Dora framework makes it easy to create dataflows for robotics and AI. In this talk, we look into distributed dataflows that run on multiple machines and communicate through the network. Dora supports complex network topologies by using the Zenoh protocol. This way, it is possible to split Dora dataflows across private networks and cloud machines with minimal configuration.",
        "speakers": [
          {
            "id": "philipp-oppermann",
            "name": "Philipp Oppermann",
            "tags": ["embodied-ai"],
            "roleOrg": "Dora Project Lead",
            "image": "philipp-oppermann.jpeg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "12:30 - 13:50",
        "title": "Spotlight Talks",
        "content": "GOSIM AI Spotlight Finalists will present their projects.",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "14:00 - 14:40",
        "title": "Adversarial Safety-Critical Scenario Generation for Autonomous Driving",
        "content": "valuating the decision-making system is indispensable in developing autonomous vehicles, while realistic and challenging safety-critical test scenarios play a crucial role. Obtaining these scenarios is non-trivial due to the long-tailed distribution, sparsity, and rarity in real-world data sets. To tackle this problem, we introduce a natural adversarial scenario generation solution using naturalistic human driving priors and reinforcement learning. Our experiments on public data sets demonstrate that our proposed model can generate realistic safety critical test scenarios covering both naturalness and adversariality with 44% efficiency gain over the baseline model.",
        "speakers": [
          {
            "id": "james-yang",
            "name": "James Yang",
            "tags": ["embodied-ai"],
            "roleOrg": "Professor at University of Science and Technology of China",
            "image": "james-yang.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "14:40 - 15:20",
        "title": "RoboBrain: A Unified Brain Model for Robotic Manipulation & RoboOS: A Hierarchical Collaborative Framework for RoboBrain and Robot Agents",
        "content": "RoboBrain is an MLLM-based model that enhances robotic manipulation by integrating task planning, object affordance, and trajectory prediction, addressing the limitations of MLLMs in robotic scenarios—particularly in long-horizon tasks—while achieving state-of-the-art performance. Building on RoboBrain’s planning capabilities, we propose the RoboOS hierarchical collaborative framework, which integrates the efficient execution of the robotic skills to enable cross-embodiment collaborative control of multiple robots.",
        "speakers": [
          {
            "id": "minglan-lin",
            "name": "Minglan Lin",
            "tags": ["embodied-ai"],
            "roleOrg": "Embodied Intelligence Researcher at BAAI",
            "image": "minglan-lin.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 6",
        "timeSlot": "15:40 - 16:20",
        "title": "Building Robotic Applications with Open-source VLA Models",
        "content": "Ville shares the successes and challenges in using open-source Vision-Language-Action (VLA) models on robots, and provides a full-stack \"starter guide\" for building VLA-powered robotic applications in 2025.",
        "speakers": [
          {
            "id": "ville-kuosmanen",
            "name": "Ville Kuosmanen",
            "tags": ["embodied-ai"],
            "roleOrg": "Founder at Voyage Robotics",
            "image": "ville-kuosmanen.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "16:20 - 17:00",
        "title": "Spatial Reasoning LLM: Enhancing 2D & 3D Understanding for Robotic Manipulation and Navigation",
        "content": "Robotic systems require advanced spatial reasoning for navigation and manipulation. We introduce a research project enhancing LLMs for spatial intelligence: AlphaMaze, solving 2D mazes with self-correction; AlphaSpace, interpreting object positions for robot hand manipulation via language; and AlphaVoxel, using 3D voxel space for object recognition and robot navigation.",
        "speakers": [
          {
            "id": "huy-hoang-ha",
            "name": "Huy Hoang Ha",
            "tags": ["embodied-ai"],
            "roleOrg": "LLM Researcher at Menlo Research",
            "image": "huy-hoang-ha.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "17:00 - 18:00",
        "title": "Spotlight Demos",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:30 - 11:10",
        "title": "How to Build Your Humanoid",
        "content": "In 2021, desperate need for human connection led me to the creation of a 16-DOF data glove. Open-sourcing it made the glove find its way to Rob Knight, creator of the 16-DOF DexHand, who had just begun developing a humanoid with Rémi Cadène's LeRobot.",
        "speakers": [
          {
            "id": "martino-russi",
            "name": "Martino Russi",
            "tags": ["embodied-ai"],
            "roleOrg": "Embodied Robotics Engineer at Hugging Face",
            "image": "martino-russi.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "11:10 - 11:50",
        "title": "G1 Open Source Dataset and Humanoid Robot from Unitree Robotics",
        "content": "With artificial intelligence technology move very fast in the past two years, humanoid robot have been one of the most import form to realized embodied AI and AGI, Unitree have been working for more than 8 years in leg robot and 1 year in humanoid robot area. There are three most important parts, algorithm, data and computing capability for realized AGI. Those three part will finally running on physical robots, we believe build robust physical humanoid robot system is key for this ecosystem, and World Large-Scale Model (most people called foundation model) is the key to bring Embodied AI for for humanoid robot, we will share the most important progressing have been made on industry and research side in the past one year, and expect and excited for new progressing will happening in next few years soon.\r\nIn order to promote the development of the global embodied AI industry, the Unitree G1 robot operation data set is open sourced, adapted to a variety of open source solutions, and continuously updated.\r\n",
        "speakers": [
          {
            "id": "min-zhang",
            "name": "Min Zhang",
            "tags": ["embodied-ai"],
            "roleOrg": "EU Director of Unitree Robotics",
            "image": "min-zhang.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "12:30 - 13:50",
        "title": "Spotlight Talks",
        "content": "GOSIM AI Spotlight Finalists will present their projects.",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "14:00 - 14:40",
        "title": "Designing Emotions for Robots",
        "content": "What happens when a robot starts to feel? We connected a language model to Reachy 2 and explored how speech can turn into emotion through gestures, sounds, and expressions. A first step toward artificial empathy in human-robot interaction.",
        "speakers": [
          {
            "id": "anne-charlotte-passanisi",
            "name": "Anne-Charlotte Passanisi",
            "tags": ["embodied-ai"],
            "roleOrg": "Senior Product Manager at Pollen Robotics",
            "image": "annecharlotte-passanisi.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "14:40 - 15:20",
        "title": "Learning from Human Demonstrations: A New Paradigm for Scalable Robot Data Acquisition",
        "content": "Acquiring diverse and large-scale real-world robot data remains a critical bottleneck in training generalizable robotic action models. Efficient and scalable data collection has thus emerged as a key research focus in robotics.\r\nA widely used method is teleoperation, where humans either wear VR devices or operate a secondary robot to guide actions. While effective, these approaches are limited by hardware-specific constraints and require complex setups, hindering scalability.\r\nAn emerging alternative is to learn directly from human demonstrations without relying on teleoperation hardware. This paradigm allows robots to acquire task-relevant motion data by observing or interpreting natural human movements, offering a more flexible and hardware-agnostic solution.\r\nIn this talk, I will introduce a novel framework for robot data acquisition from human demonstrations. I will detail how it bypasses traditional teleoperation limitations and enables scalable learning across varied tasks and environments. By bridging the gap between human intent and robot execution, this method opens a promising direction for general-purpose robotic learning in the real world.",
        "speakers": [
          {
            "id": "junkai-zhao",
            "name": "Junkai Zhao",
            "tags": ["embodied-ai"],
            "roleOrg": "Robot Algorithm Engineer at BAAI",
            "image": "junkai-zhao.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "15:40 - 16:20",
        "title": "AI Empowers IoT Devices to Drive the Dual Engines of Industrial Transformation",
        "content": "Amidst the contemporary surge of digital transformation, the symbiotic convergence of artificial intelligence (AI) and IoT devices has emerged as a pivotal catalyst for industrial evolution. AI's infusion of autonomous learning, intelligent decision-making, and seamless interaction capabilities into intelligent hardware has redefined the paradigm, elevating conventional tools to the status of sophisticated, intelligent collaborators. This technological metamorphosis is evident across a spectrum of applications, from the bespoke experiences delivered by smart home ecosystems to the pinpoint precision of operations within industrial automation frameworks. The ramifications of this fusion extend beyond mere enhancement; it has become a driving force propelling the digital reinvention of traditional industries and the emergence of new sectors. In this presentation, we will delve into the intricate dynamics of the integration trends between AI and IoT devices, explore groundbreaking technological innovations, examine a diverse array of application scenarios, and assess the profound and far-reaching impacts on industrial transformation. By doing so, we aim to peer into the future, where the potential for growth and innovation is boundless, and to chart a course that offers novel insights and strategic directions for the continued advancement of our industry.",
        "speakers": [
          {
            "id": "jack-huang",
            "name": "Jack Huang",
            "tags": ["embodied-ai"],
            "roleOrg": "Chairman & CEO at Gizwits",
            "image": "jack-huang.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "16:20 - 17:00",
        "title": "Towards a Cerebrum-Cerebellum Collaboration Framework for Large Embodied Models",
        "content": "Recent advances in large-scale multimodal models have significantly enhanced embodied AI systems, enabling more natural and adaptive interactions with the physical world. However, current models and frameworks often struggle with spatial-temporal perception, real-time and precise collabration. Inspired by the biological synergy between the cerebrum and cerebellum, we propose a novel collaboration framework that integrates high-level cognitive reasoning with fast, low-latency sensorimotor adaptations. This talk explores how this framework can improve planning, error correction, and robustness in embodied AI. We will discuss model architectures, training strategies, and applications in robotic manipulation and human-robot interaction, paving the way for more agile and intelligent embodied systems.",
        "speakers": [
          {
            "id": "cheng-chi",
            "name": "Cheng Chi",
            "tags": ["embodied-ai"],
            "roleOrg": "Researcher at BAAI",
            "image": "cheng-chi.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "17:00 - 18:00",
        "title": "Spotlight Demos",
        "content": "",
        "speakers": []
      }
    ],
    "PyTorch Day France": [
      {
        "date": "May 7",
        "timeSlot": "9:30 - 10:00",
        "room": "Master Stage",
        "title": "Keynote",
        "content": "GOSIM Keynote",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:00 - 10:30",
        "title": "Morning Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "10:30 - 10:50",
        "title": "Welcome & Opening Remarks",
        "content": "",
        "speakers": [
          {
            "id": "matt-white",
            "name": "Matt White",
            "tags": ["pytorch"],
            "roleOrg": "Executive Director, PyTorch Foundation. GM of AI, Linux Foundation.",
            "image": "matt-white.png"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "10:50 - 11:10",
        "title": "Real-World Robotics as the Next Frontier for AI?",
        "content": "As AI continues to push the boundaries of perception and decision-making, robotics emerges as one of its most exciting and demanding playground. In this talk, we’ll explore how the intersection of machine learning and robotics opens up powerful avenues for interaction, manipulation, and embodied intelligence. We will emphasize the critical role of real-world experimentation and data collection in bridging the gap between simulation and deployment. Interestingly, tasks traditionally viewed as complex, like locomotion, have seen significant progress, while seemingly simple behaviors—such as dexterous manipulation—remain open challenges. By grounding AI systems in physical environments, we gain deeper insight into their capabilities and limitations, and identify new directions for research at the intersection of learning, control, and embodiment.",
        "speakers": [
          {
            "id": "pierre-rouanet",
            "name": "Pierre Rouanet",
            "tags": ["pytorch"],
            "roleOrg": "Co-Founder & CTO at Pollen Robotics",
            "image": "pierre-rouanet.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "11:10 - 11:30",
        "title": "TorchCodec: The Media Decoding Library for PyTorch",
        "content": "TorchCodec is a new PyTorch library for decoding video and audio data into tensors, on CPU and CUDA GPU. It aims to be fast, easy to install, easy to use, and well integrated into the PyTorch ecosystem. In this talk, we’ll present the various decoding capabilities of TorchCodec, how to sample video frames, and we’ll describe more advanced use-cases like streaming videos from the cloud.",
        "speakers": [
          {
            "id": "nicolas-hug",
            "name": "Nicolas Hug",
            "tags": ["pytorch"],
            "roleOrg": "ML Research Engineer at Meta",
            "image": "nicolas-hug.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "11:30 - 11:50",
        "title": "Scaling LLM Inference with vLLM: Multi‑Accelerator Serving and Quantized LLMs",
        "content": "vLLM has become the community-standard engine for low-latency LLM inference, achieving a 10× increase in usage in 2024 and surpassing 100,000 daily installs by January 2025. Supported by hundreds of contributors and productized through Red Hat AI, vLLM provides a vendor-neutral solution for serving cutting-edge models at scale. This talk outlines a practical blueprint for scaling LLM inference using vLLM, integrating both system-level techniques and model-level optimizations.\n\n We begin by addressing the challenges of deploying LLMs with chain-of-thought reasoning in production. Leveraging vLLM’s engine architecture, multi-accelerator deployments using tensor parallelism, paged attention scheduling, and prefill–decode disaggregation demonstrate how a single node can efficiently drive multiple AI accelerators, enhancing throughput without compromising latency.\n\n The second optimization layer focuses on quantization. Based on over 500,000 evaluations across language and vision-language models, we examine the accuracy–speed trade-offs of weight and activation quantization. We introduce new pathways that significantly reduce memory usage while maintaining model quality. Attendees will leave with data-driven insights and ready-to-use configurations for deploying state-of-the-art quantized models in scalable enterprise inference pipelines.",
        "speakers": [
          {
            "id": "erwan-gallen",
            "name": "Erwan Gallen",
            "tags": ["pytorch"],
            "roleOrg": "Senior Principal Product Manager at RedHat",
            "image": "erwan-gallen.jpeg"
          },
          {
            "id": "eldar-kurtic",
            "name": "Eldar Kurtic",
            "tags": ["pytorch"],
            "roleOrg": "Senior ML Research Engineer at RedHat",
            "image": "eldar-kurtic.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "12:00 - 14:00",
        "room": "Open Platform",
        "title": "Lunch Break",
        "content": "Food & drinks",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "14:00 - 14:20",
        "title": "Llama 4",
        "content": "This presentation explores the development of Llama 4, a state-of-the-art foundation model designed to excel in various tasks. We will discuss its key features, including long=context and multimodal understanding.\n We will also examine Llama 4's potential uses in agentic settings, such as autonomous decision-making and human-AI collaboration, through real-world examples and case studies.",
        "speakers": [
          {
            "id": "christian-keller",
            "name": "Christian Keller",
            "tags": ["pytorch"],
            "roleOrg": "Product Lead - Generative AI Research at Meta",
            "image": "christian-keller.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "14:20 - 14:40",
        "title": "Harnessing Common Crawl for AI and ML applications",
        "content": "This presentation looks at effective strategies for using Common Crawl's web archive in large-scale research applications, specifically for AI and other ML applications. We will discuss practical approaches to processing and filtering Common Crawl’s datasets, with focus on how to overcome computational challenges and optimise data pipelines. We will also discuss some of the challenges that users might encounter related to the multilingual and heterogeneous nature of Common Crawl’s data. The talk will cover best practices for data filtering, pre-processing, and storage, to ensure the quality and relevance of extracted information for research tasks. Additionally, we will briefly discuss the ranking mechanism used to determine whether a URL is crawled, and demonstrate how to use the Web Graph as a framework for further research.",
        "speakers": [
          {
            "id": "pedro-ortis",
            "name": "Pedro Ortis",
            "tags": ["pytorch"],
            "roleOrg": "Senior Research Scientist at Common Crawl",
            "image": "pedro-ortis-suarez.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "14:40 - 15:00",
        "title": "The Ultra-Scale Talk: Scaling Training to Thousands of GPUs",
        "content": "Training large language models (LLMs) demands more than just raw compute—it requires infrastructure, strategy, and a deep understanding of parallelism. What begins as a single-GPU prototype must eventually scale across thousands of devices, each step introducing new complexity.\n\n This talk dives into the practicalities of ultra-scale training. We'll explore how 5D parallelism—spanning data, tensor, pipeline, context, and expert dimensions—makes it possible to stretch a single training run across massive GPU clusters. Along the way, we’ll cover performance tuning, communication patterns, and architecture choices that impact throughput and hardware efficiency.\n\n A key reference for this session is the Ultra-Scale Playbook, which distills best practices and hard-earned lessons from real-world LLM scaling efforts. We’ll walk through highlights of the playbook, tying them into case studies, benchmarks, and hands-on recommendations.\n\n Scaling isn’t just about size—it’s about doing more with what you have. This webinar offers a comprehensive look at what it really takes to train state-of-the-art models at scale, designed for engineers, researchers, and practitioners ready to move beyond “it fits on one GPU” toward infrastructure that powers trillion-parameter models—efficiently, and at speed.",
        "speakers": [
          {
            "id": "nouamane-tazi",
            "name": "Nouamane Tazi",
            "tags": ["pytorch"],
            "roleOrg": "Machine Learning Engineer at Hugging Face",
            "image": "nouamane-tazi.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "15:00 - 15:20",
        "title": "Teaching Mistral to Reason: Post-Training with PyTorch and NVIDIA",
        "content": "Post-training techniques have become essential as demand for Reasoning AI systems explodes. This talk provides a practical overview of how to enhance the reasoning capabilities of open-weight models—using Mistral as a working example. We’ll explore the full pipeline: sourcing high-quality reasoning datasets, selecting the right model checkpoints, and using tools that extend the functionality of PyTorch like NVIDIA NeMo and TensorRT-LLM. Whether you’re working on chatbots, agents, or task-specific models, you’ll leave with a clear understanding of the tools and workflows to take advantage of open models.",
        "speakers": [
          {
            "id": "meriem-bendris",
            "name": "Meriem Bendris",
            "tags": ["pytorch"],
            "roleOrg": "Conversational AI Solutions Architect Manager at NVIDIA",
            "image": "meriem-bendris.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "15:20 - 15:40",
        "title": "Afternoon Coffee",
        "content": "Coffee & hors d’oeuvres",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "15:40 - 16:00",
        "title": "DeepSpeed – Efficient Training Scalability for Deep Learning Models",
        "content": "Deep Learning (DL) is driving unprecedented progress across Artificial Intelligence domains, including natural language processing, vision, speech, and multimodal. Sustaining this rapid pace of AI revolution, however, requires practical solutions to the extreme demands of scaling on the compute, memory, communication, and storage components of modern computing hardware. To address this challenge, we created a deep learning optimization library called DeepSpeed to make distributed model training efficient, effective, and easy on commodity hardware. This talk will focus on DeepSpeed optimizations for improving compute, communication, and I/O of extreme-scale model training.",
        "speakers": [
          {
            "id": "olatunji-ruwase",
            "name": "Olatunji Ruwase",
            "tags": ["pytorch"],
            "roleOrg": "Deep Learning Expert",
            "image": "olatunji-ruwase.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "16:00 - 16:20",
        "title": "Advancing Mamba in PyTorch",
        "content": "Mamba layers are efficient alternatives to standard attention: their training complexity is linear in sequence length, while inference is sequence-length-independent and only requires a small cache. I will discuss a selection of IBM's ongoing work in advancing the state of mamba training in pytorch, including: context-parallel training for long-sequence data, mamba + mixture-of-expert support with expert parallelism, torch-native associative scan ops, and improved DTensor op support.",
        "speakers": [
          {
            "id": "garrett-goon",
            "name": "Garrett Goon",
            "tags": ["pytorch"],
            "roleOrg": "Research Scientist at IBM",
            "image": "garrett-goon.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "16:20 - 16:40",
        "title": "Lightning Thunder: Supercharged PyTorch for Modern Hardware",
        "content": "Modern GPUs like Hopper and Blackwell are fast, but only after careful optimization. Lightning Thunder compiles “education-style” PyTorch models into optimized, distributed PyTorch code. Through a composable plugin system, Lightning Thunder lets developers layer in kernel fusion, low-precision operations, memory optimizations, and flexible parallelism strategies, to achieve performance and scale while leaving the original PyTorch code unchanged. This talk will cover how Lightning Thunder bridges the gap between ease-of-use and peak performance, and enables teams to easily write custom code transformations to scale models efficiently, reduce GPU waste, and stay in control of their stack.",
        "speakers": [
          {
            "id": "luca-antiga",
            "name": "Luca Antiga",
            "tags": ["pytorch"],
            "roleOrg": "CTO at Lightning AI",
            "image": "luca-antiga.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "16:40 - 17:00",
        "title": "To Be Announced",
        "content": "",
        "speakers": []
      },
      {
        "date": "May 7",
        "timeSlot": "17:00 - 17:20",
        "title": "Best Practices for Open Multilingual LLM Evaluation",
        "content": "Multilingual language models seem to be getting better, but how do we know? In general, language model evaluation is made more uncertain by automatic evaluations which correlate poorly with human ratings, low-quality datasets, and a lack of reproducibility. But for languages other than high-resource languages like English and Mandarin Chinese, these problems are even more consequential. We provide a set of best practices for using existing evaluations. Given the limited number of evaluations for many languages, we highlight languages and tasks that need more benchmarks and outline key considerations for developing new multilingual benchmarks.",
        "speakers": [
          {
            "id": "catherine-arnett",
            "name": "Catherine Arnett",
            "tags": ["pytorch"],
            "roleOrg": "NPL Researcher at EleutherAI",
            "image": "catherine-arnett.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "17:20 - 17:40",
        "title": "Multimodal Open Source at Kyutai, From Online Demos to On-Device",
        "content": "Kyutai is a leading lab for open science and open source research in Paris. During this session, we dive in some of our recent open source releases, from hosted demos to model running streaming on device, covering multilingual LLM, vision-speech models, simultaneous interpretation and more. We will discuss the challenges arising from supporting a large number of applications and inference platform, with and without PyTorch.",
        "speakers": [
          {
            "id": "alexandre-defossez",
            "name": "Alexandre Défossez",
            "tags": ["pytorch"],
            "roleOrg": "Chief Exploration Officer at Kyutai",
            "image": "alexandre-defossez.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "17:40 - 18:00",
        "title": "PyTorch x Transformers journey: pythonicity, autodiff and modularity defining modern AI",
        "content": "The HuggingFace Transformers library is a flagship example of what makes PyTorch special: a dynamic, readable, and hackable framework that scales from quick experiments to production-ready architectures. It began as an implementation of BERT, continued to a \"\"one model, one file\"\" setup—ideal for iteration—and grew into a modular codebase now defining 315+ models. Transformers has become a reference implementation for the field: a source of truth for model architectures, behaviors, and pretraining conventions. Its evolution reflects PyTorch’s own: grounded in Pythonic values, but pragmatic enough to diverge when needed.\n\n PyTorch’s ecosystem has replaced entire toolchains. Scaling models has become simpler: torch.compile brings compiler-level speedups with minimal code changes, and new abstractions like DTensor offer serious performance gains without the low-level complexity.\n\n Both PyTorch and Transformers inherit Python’s spirit—clarity, flexibility, expressiveness—without being bound by it. PyTorch leans on ATen and C++ kernels under the hood; Transformers increasingly rely on optimized community kernels and hardware-aware implementations from the hub.\n\n Modularity and readability didn’t just improve maintainability—they grew the community. Lowering the barrier to entry encourages experimentation, contributions, and faster innovation. This talk tracks that journey—from how PyTorch enabled Transformers, to how the virtuous cycle of design, performance, and pragmatism continues to shape the tools driving modern AI.",
        "speakers": [
          {
            "id": "pablo-montalvo",
            "name": "Pablo Montalvo",
            "tags": ["pytorch"],
            "roleOrg": "Machine Learning Engineer at Hugging Face",
            "image": "pablo-montalvo.jpeg"
          }
        ]
      }
    ],
    "GOSIM AI Spotlight": [
      {
        "date": "May 6",
        "timeSlot": "12:30 -12:45",
        "title": "Aidge",
        "content": "In an era increasingly defined by intelligent systems at the edge, Eclipse Aidge emerges as a crucial open-source initiative to democratize the development and deployment of efficient Deep Neural Networks (DNN) on embedded platforms. Recognizing the unique constraints and opportunities of edge computing – limited resources, real-time requirements, and the need for robust, low-power AI – Aidge provides a comprehensive framework for addressing this complex task. By offering end-to-end tools for DNN design, analysis, simulation, optimization and hardware-aware deployment across diverse targets (CPUs, GPUs, MCUs), Aidge empowers developers to build sophisticated AI applications for IoT, robotics, automotive, and beyond. This project fosters innovation by enabling rapid prototyping, performance benchmarking, and the creation of tailored solutions, ultimately accelerating the adoption of intelligent edge devices while promoting interoperability, transparency and collaboration within the open-source ecosystem. Key features : - High interoperability with the ONNX standard (import and export of the DNN model) - Rich tools for static analysis of the DNN model - State-of-the-art quantization and compression techniques to reduce the memory needs - Standalone C++ code generation of the DNN Some planned features : - A certification-aware code generation module. - Integration of a scalable and efficient ML compilation tool chain based on MLIR or TVM - Extension of Spike Neural Networks to be the first multi-paradigm framework natively addressing neuromorphic computing. Eclipse Aidge is hosted on The Eclipse Foundation's GitLab instance.",
        "speakers": [
          {
            "id": "cyril-moineau",
            "name": "Cyril Moineau",
            "tags": ["spotlight-ai"],
            "roleOrg": "Research Engineer at CEA",
            "image": "cyril-moineau.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "12:45 -13:00",
        "title": "SpeakLeash",
        "content": "The Bielik Project delivers not only a family of open-source language models (1.5B, 4.5B, and 11B parameters) but also a complete tooling ecosystem designed to empower others to train, fine-tune, and evaluate LLMs with ease. One of the project’s core features is its end-to-end toolchain—spanning datasets, benchmarking, training, and fine-tuning frameworks—which enables any research group or organization to build their own models through base model fine-tuning or continuous pretraining.",
        "speakers": [
          {
            "id": "michal-domaski",
            "name": "Michał Domański",
            "tags": ["spotlight-ai"],
            "roleOrg": "Bielik.ai",
            "image": "generic-profile.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "13:00 -13:15",
        "title": "Running GenAI Models on Edge Devices with LlamaEdge",
        "content": "Run GenAI models on edge devices with LlamaEdge: multi-runtime support (llama.cpp, Torch, ONNX, OpenVINO), small footprint, GPU support, Rust SDK embeddability. Learn how it outperforms tools like Ollama for flexible, fast, on-device AI deployment.",
        "speakers": [
          {
            "id": "hung-ying-tai",
            "name": "Hung-Ying Tai",
            "tags": ["spotlight-ai"],
            "roleOrg": "Tech Lead at Second State",
            "image": "hung-ying-tai.jpg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "13:15 -13:30",
        "title": "Automated Proof Generation for Rust Code Via Self-Evolution",
        "content": "Ensuring correctness is crucial for code generation. Formal verification offers a definitive assurance of correctness, but demands substantial human effort in proof construction and hence raises a pressing need for automation. The primary obstacle lies in the severe lack of data—there are much fewer proofs than code snippets for Large Language Models (LLMs) to train upon. In this paper, we introduce SAFE, a framework that overcomes the lack of human-written proofs to enable automated proof generation of Rust code. SAFE establishes a self-evolving cycle where data synthesis and fine-tuning collaborate to enhance the model capability, leveraging the definitive power of a symbolic verifier in telling correct proofs from incorrect ones. SAFE also re-purposes the large number of synthesized incorrect proofs to train the self-debugging capability of the fine-tuned models, empowering them to fix incorrect proofs based on the verifier’s feedback. SAFE demonstrates superior efficiency and precision compared to GPT-4o. Through tens of thousands of synthesized proofs and the self-debugging mechanism, we improve the capability of open-source models, initially unacquainted with formal verification, to automatically write proofs for Rust code. This advancement leads to a significant improvement in performance, achieving a 52.52% accuracy rate in a benchmark crafted by human experts, a significant leap over GPT-4o’s performance of 14.39%.",
        "speakers": [
          {
            "id": "tianyu-chen",
            "name": "Tianyu Chen",
            "tags": ["spotlight-ai"],
            "roleOrg": "Ph.D Candidate at Peking University",
            "image": "tianyu-chen.jpeg"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "13:30 -13:45",
        "title": "ColPali: Efficient Document Retrieval with Vision Language Models",
        "content": "This repository contains the code used for training the vision retrievers in the ColPali: Efficient Document Retrieval with Vision Language Models paper. In particular, it contains the code for training the ColPali model, which is a vision retriever based on the ColBERT architecture and the PaliGemma model.",
        "speakers": [
          {
            "id": "gautier-viaud",
            "name": "Gautier Viaud",
            "tags": ["spotlight-ai"],
            "roleOrg": "Head of Research & Development at Illuin Technology",
            "image": "generic-profile.png"
          }
        ]
      },
      {
        "date": "May 6",
        "timeSlot": "13:45 -14:00",
        "title": "Unlocking Heterogeneous AI Infrastructure K8s Cluster: Leveraging the Power of HAMi",
        "content": "Unlocking the full potential of AI Infra heterogeneous GPUs with HAMi for efficient scheduling, unified management, observability, and utilization improvement",
        "speakers": [
          {
            "id": "xiao-zhang",
            "name": "Xiao Zhang",
            "tags": ["spotlight-ai"],
            "roleOrg": "CEO of Dynamia.ai",
            "image": "xiao-zhang.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "12:30 -12:45",
        "title": "Code Carbon",
        "content": "With AI models becoming more ubiquitous and deployed across different sectors and industries, AI’s environmental impact is also growing. Thus, state-of-the-art Machine Learning models leverage significant amounts of computing power, training on advanced processors for weeks or months, consequently consuming enormous amounts of energy. Depending on the energy grid used during this process, this can entail the emission of large amounts of greenhouse gases such as CO₂. For this reason, it is important to estimate and curtail both the energy used and the emissions produced by training and deploying AI models. This package enables developers to track carbon dioxide (CO₂) emissions across machine learning experiments or other programs. Code Carbon enables developers to track emissions, measured as kilograms of CO₂-equivalents (CO₂eq) in order to estimate the carbon footprint of their work. For this purpose, we use CO₂-equivalents [CO₂eq], which is a standardized measure used to express the global warming potential of various greenhouse gases: the amount of CO₂ that would have the equivalent global warming impact. For computing, which emits CO₂ via the electricity it is consuming, carbon emissions are measured in kilograms of CO₂-equivalent per kilowatt-hour. As a matter of fact, electricity is generated as part of the broader electrical grid by combusting fossil fuels for example.",
        "speakers": [
          {
            "id": "benoit-courty",
            "name": "Benoit Courty",
            "tags": ["spotlight-ai"],
            "roleOrg": "Neo-Robotix",
            "image": "benoit-courty.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "12:45 -13:00",
        "title": "Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation",
        "content": "To accelerate continuous skill acquisition through reusable motion-aware primitives, we propose Primitive Prompt Learning (PPL). PPL enables lifelong robot manipulation by optimizing new prompts alongside pretrained ones and demonstrates superior performance in both simulated and real-world tasks.",
        "speakers": [
          {
            "id": "siao-liu",
            "name": "Siao Liu",
            "tags": ["spotlight-ai"],
            "roleOrg": "Researcher at BAAI",
            "image": "siao-liu.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "13:00 -13:15",
        "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation Via Dynamic Iterative Reasoning Agents",
        "content": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents. We propose to add automatic differentiation to Rust. This would allow Rust users to compute derivatives of arbitrary functions, which is the essential enabling technology for differentiable programming. This feature would open new opportunities for Rust in scientific computing, machine learning, robotics, computer vision, probabilistic analysis, and other fields",
        "speakers": [
          {
            "id": "ruixue-ding",
            "name": "Ruixue Ding",
            "tags": ["spotlight-ai"],
            "roleOrg": "Algorithm Expert at Alibaba Tongyi Lab",
            "image": "ruixue-ding.jpg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "13:15 -13:30",
        "title": "LlamaFactory",
        "content": "LlamaFactory is an easy-to-use and efficient framework for fine-tuning large language models. It can fine-tune hundreds of pre-trained models locally without writing any code. It supports popular models like Llama, Mistral, Qwen, Phi, etc. It streamlines the training process of LLMs from (continuous) pre-training, supervised fine-tuning to RLHF, accompanying with lower costs and better efficiency. By integrating GPTQ quantization and LoRA method, it can fine-tune a 30B model on a single RTX4090 GPU. It further supports flash attention and vLLM to accelerate the training and inference of LLMs.",
        "speakers": [
          {
            "id": "yaowei-zheng",
            "tags": ["spotlight-ai"],
            "name": "Yaowei Zheng",
            "roleOrg": "Ph.D. Student at Beihang University",
            "image": "yaowei-zheng.jpeg"
          }
        ]
      },
      {
        "date": "May 7",
        "timeSlot": "13:30 -13:45",
        "title": "MoFA",
        "content": "MoFA (Modular Framework for Agent) is a software framework designed to build AI agents through a composition-based approach. MoFA stands out in several key ways: Empowering Ordinary People to Do Extraordinary Things: AI should not be the exclusive domain of elites and large corporations. MoFA empowers everyone to harness and develop AI, turning the impossible into possible and enabling ordinary people to create extraordinary outcomes. A Fundamental Approach of Composition: Inspired by the Unix philosophy, MoFA centers \"composition\" as its core principle. You can build agents, connect them, and integrate tools like building blocks, making AI simple, flexible, and powerful. The Unified System of Everything Agent: In MoFA, an agent is not just a large language model—it can be code, scripts, APIs, or even MoFA itself. MoFA is more than a framework; it is an entire agent ecosystem, where the agent is the application of the AI era. Data Flow Over Workflow in Architecture: While most agent frameworks rely on complex and stateful workflows, MoFA adopts a data-driven dataflow approach. This design allows agents to be easily combined, disassembled, and reused, offering greater flexibility and efficiency, relatively more friendly to debug, especially when there are hundreds of agents interacting with each other in a system.",
        "speakers": [
          {
            "id": "zonghuan-wu",
            "name": "Zonghuan Wu",
            "tags": ["spotlight-ai"],
            "roleOrg": "Relevant Study",
            "image": "generic-profile.png"
          }
        ]
      }
    ]
  }
}
